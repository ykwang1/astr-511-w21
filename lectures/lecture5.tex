\documentclass[letterpaper,landscape]{slides}
%\documentclass[letterpaper,portrait]{slides}
\usepackage{boxedminipage}

%\input /u/rhl/TeX/pdf.tex
\input pdf.tex

\newif\ifTalk\Talktrue		% We generating a talk, not printing
%\Talkfalse			% no; we're really printing

%\pagestyle{empty}
\setlength{\topmargin}{-1in}
\setlength{\textheight}{7.5in}
\setlength{\textwidth}{9in}
\setlength{\oddsidemargin}{0pt}
\setlength{\oddsidemargin}{0pt}

%\onlyslides{1-3,4,10-9999}
%\onlyslides{26-9999}

\begin{document}

\newcommand{\XXX}[1]{\textbf{XXX} #1}
\newcommand{\colour}[1]{\color{#1}}

\def\eq#1{\begin{equation} \color{blue} #1 \end{equation}}

\def\b#1{{\bf  #1}}
\def\p{\partial}
\def\th{^{th}}
\def\msun{{\rm\,M_\odot}}
\def\bnabla{{\bf\nabla}}
\def\dint{\int\!\!\!\int}
\def\d{{\rm d}}
\def\i{{\rm i}}
\def\ddt#1{{\rm{d} #1\over {\rm dt}}}
\def\ddtS#1{{\rm{d^2} #1\over {\rm dt^2}}}
%\lta and \gta produce > and < signs with twiddle underneath
\def\spose#1{\hbox to 0pt{#1\hss}}
\def\lta{\mathrel{\spose{\lower 3pt\hbox{$\mathchar"218$}}
     \raise 2.0pt\hbox{$\mathchar"13C$}}}
\def\gta{\mathrel{\spose{\lower 3pt\hbox{$\mathchar"218$}}
     \raise 2.0pt\hbox{$\mathchar"13E$}}}
\def\mspace{\hbox{\quad}}

\def\deffn#1{{\bf#1}}\def\eqs#1{equations \rf#1}


\newcount\itemCnt\itemCnt=0
\newcommand{\nitem}{%
  \global\advance\itemCnt by 1
  ~\vskip0cm\the\itemCnt.\qquad}

\definecolor{orange}{rgb}{1.0, 0.5, 0.0}
\definecolor{purple}{cmyk}{0.4, 0.8, 0.3, 0.0}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\onepic}[6]{%
\begin{slide}
     \begin{center}
        \begin{minipage}{#1in}
            {\large \color{blue} #6}
            \phantom{x} \vskip #2in
            \phantom{x} \hskip #3in
            {\scalebox{#4}{\includegraphics{#5}}}   
        \end{minipage}
     \end{center}
    \vfill
\end{slide}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\picslide}[7]{%
  \begin{slide}
     \begin{center}
        \begin{minipage}{#5in}
            \hskip #6in
            \hskip -1in
            {\scalebox{#4}{\includegraphics{#1.#2}}}
            \vskip #7in~
            {\large \color{blue} #3}
        \end{minipage}
     \end{center}
     \vfill
  \end{slide}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 

%------------------------------------------------------------------------------
%------------------------------------------------------------------------------

\begin{slide}

\phantom{x}
\vskip -2in
\begin{center}
\bfseries
{\large {\color{blue} Astr 511: Galaxies as galaxies}}
\end{center}

\input{header.tex}

\vskip 1.6in

{\centerline {\huge {\color{red}      Lecture 5:             }}}
\vskip 0.2in 
{\centerline {\Large {\color{blue}  Luminosity  and mass functions of galaxies: II  }}}

\vfill
\end{slide}
%------------------------------------------------------------------------------



%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} Outline}}
\end{center}
\vskip 0.2in
\hrule

\begin{itemize}
\item Luminosity function: basic concepts
\item Stellar mass function in the Milky Way 
\item {\bf Methods for estimating LF from data} 
\end{itemize}
%
%
\vfill
\end{slide}


%------------------------------------------------------------------------------
% TWO-SIDED PAGE 
\begin{slide}

\hbox to \hsize{
\begin{minipage}[t]{9cm}
\begin{center}
\vskip -5.4in
\scalebox{2.0}{\hskip -2.1in \includegraphics{figures/fig_lyndenbell_toy-2.pdf}}
\end{center}
\end{minipage}

\begin{minipage}[t]{15cm}
\begin{itemize}
\item
Toy example: the x-y ``measurements'' are truncated (similar to a flux-limited
sample in astronomy). 
\item
Histograms of measured quantities are biased for a truncated sample. 
\item
Unbiased distributions can be obtained using various methods ($V/V_{max}$ method,
C$^-$ method, maximum likelihood method) 
\end{itemize}  


\end{minipage}}
\vfill 
\end{slide}
%------------------------------------------------------------------------------




%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} The C$^-$ method for estimating LF}}
\end{center}
\vskip 0.2in
\hrule

\begin{itemize}
\item Lynden-Bell (1971, MNRAS 155, 95); a non-parametric method that works 
for separable LFs, $\Psi(L,z) = \Phi(L) n(z)$
\item practically all non-parametric methods can be reduced to the $C^-$
      method (Petrosian 1992)
\item parametric methods are usually based on maximizing likelihood
      (e.g. Marshall 1985)
\item the simplest and most famous method, the $V_{max}$ method (Schmidt 1968),
      requires binning in two axes simultaneously, while with the $C^-$
      method data is binned only one axis at a time (e.g. Fan et al. 2001) 
\item How do we know that separable LF is a good guess for our data?
\end{itemize}

\vfill
\end{slide}
 




%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} C$^-$ method}}
\end{center}
\vskip 0.2in
\hrule

\begin{itemize}
\item
Given a set of measured pairs $(x_i, y_i)$, with 
$i=1 \dots N$, and {\it known} relation $y_{max}(x)$, estimate the two-dimensional
distribution, $n(x,y)$, from which the sample was drawn. Assume that 
measurement errors for both $x$ and $y$ are negligible compared to their observed 
ranges, that $x$ is measured within a range defined by $x_{min}$ and $x_{max}$, 
and that the selection function is 1 for $0 \le y\le y_{max}(x)$ and $x_{min} \le x \le x_{max}$,  
and 0 otherwise.
\end{itemize}

\vskip -0.7in
\phantom{x}
\scalebox{0.9}{\hskip 1.4in \includegraphics{figures/fig_lyndenbell_setup-1.pdf}}

\vfill
\end{slide}
 

%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} C$^-$ method}}
\end{center}
\vskip 0.2in
\hrule

\begin{itemize}
\item
$C^-$ method is applicable when the distributions along the two coordinates $x$ and $y$ 
are uncorrelated, that is, when we can assume that the bivariate distribution $n(x,y)$ is separable
\eq{
\label{eq:separable2D}
                    n(x,y) = \Psi(x) \, \rho(y). 
}
Therefore, before using the $C^-$ method we need to demonstrate that this
assumption is valid. 
\end{itemize}

\vskip -0.7in
\phantom{x}
\scalebox{0.9}{\hskip 1.4in \includegraphics{figures/fig_lyndenbell_setup-1.pdf}}

\vfill
\end{slide}
 

%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} C$^-$ method}}
\end{center}
\vskip 0.2in
\hrule

\begin{itemize}
\item 
Define a {\it comparable} or {\it associated} set for each object $i$ such that 
$J_i = \{ j:x_j < x_i, y_j < y_{max}(x_i)\}$; this is the largest $x$-limited and $y$-limited data 
subset for object $i$, with $N_i$ elements (see the left panel).
\item Sort the set $J_i$ by $y_j$; this gives us the rank $R_j$ for each object (ranging
from 1 to $N_i$)
\end{itemize}

\vskip -0.7in
\phantom{x}
\scalebox{0.9}{\hskip 1.4in \includegraphics{figures/fig_lyndenbell_setup-1.pdf}}

\vfill
\end{slide}
 

%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} C$^-$ method}}
\end{center}
\vskip 0.2in
\hrule

\begin{itemize}
\item 
Define a {\it comparable} or {\it associated} set for each object $i$ such that 
$J_i = \{ j:x_j < x_i, y_j < y_{max}(x_i)\}$; this is the largest $x$-limited and $y$-limited data 
subset for object $i$, with $N_i$ elements.
\item Sort the set $J_i$ by $y_j$; this gives us the rank $R_j$ for each object (ranging
from 1 to $N_i$)
\item Define the rank $R_i$ for object $i$ in {\it its} associated set: this is 
essentially the number of objects with $y<y_i$ in set $J_i$.
\item If $x$ and $y$ are truly independent, $R_i$ must be distributed 
      {\it uniformly} between 0 and $N_i$. 
\end{itemize}

\vskip -0.7in
\phantom{x}
%\scalebox{0.9}{\hskip 1.4in \includegraphics{figures/fig_lyndenbell_setup-1.pdf}}

\vfill
\end{slide}
 


%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} C$^-$ method}}
\end{center}
\vskip 0.2in
\hrule

\begin{itemize}
\item If $x$ and $y$ are truly independent, $R_i$ must be distributed 
      {\it uniformly} between 0 and $N_i$.
\item In this case, it is trivial to determine 
      the expectation value and variance for $R_i$: $E(R_i) = E_i = N_i/2$
      and $V(R_i) = V_i = N_i^2/12$. We can define the statistic
\begin{equation}
      \tau = {\sum_i (R_i - E_i)\over \sqrt {\sum_i V_i   }   } 
\end{equation}
If $\tau < 1$, then $x$ and $y$ are uncorrelated at $\sim1 \sigma$ level.
\item A quiz question for in-class discussion: how would the above expression 
   for $\tau$ look like if the uniform distribution were replaced by a Gaussian 
   (normal) distribution? 
\end{itemize}

\vskip -0.7in
\phantom{x}
%\scalebox{0.9}{\hskip 1.4in \includegraphics{figures/fig_lyndenbell_setup-1.pdf}}

\vfill
\end{slide}
 



%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} C$^-$ method}}
\end{center}
\vskip 0.2in
\hrule

Assuming that $\tau<1$, it is straightforward to show using relatively simple probability integral 
analysis (e.g., see Appendix in Fan et al. 2001), as well as the original Lynden-Bell's paper,
how to determine cumulative distribution functions.
The cumulative distributions are defined as 
\eq{
             \Phi(x) = \int_{-\infty}^{x} \Psi(x') dx',
}
and 
\eq{
              \Sigma(y) = \int_{-\infty}^y \rho(y') dy'.
}
Then, 
\eq{
              \Phi(x_i) = \Phi(x_1) \, \Pi_{k=2}^i (1 + 1/N_k)
}
where it is assumed that $x_i$ are sorted ($x_1 \le x_k \le x_N$).

\vskip -0.7in
\phantom{x}
%\scalebox{0.9}{\hskip 1.4in \includegraphics{figures/fig_lyndenbell_setup-1.pdf}}

\vfill
\end{slide}
 

%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} C$^-$ method}}
\end{center}
\vskip 0.2in
\hrule


Analogously, if $M_k$ is the
number of objects in a set defined by $J_k = \{ j:y_j < y_k, y_{max}(x_j) > y_k\}$
(see the right panel of figure below), then
\eq{
              \Sigma(y_j) = \Sigma(y_1) \, \Pi_{k=2}^j (1 + 1/M_k). 
}

\vskip -0.7in
\phantom{x}
\scalebox{0.9}{\hskip 1.4in \includegraphics{figures/fig_lyndenbell_setup-1.pdf}}


\vfill
\end{slide}
 


%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} astroML implementation of the C$^-$ method}}
\end{center}
\vskip 0.2in
\hrule

\begin{itemize}
\item 
Note that both $\Phi(x_j)$ and $\Sigma(y_j)$ are defined on non-uniform grids with $N$ values,
corresponding to the $N$ measured values. 
\item 
Essentially, the $C^{-}$ method assumes a piece-wise
constant model for $\Phi(x)$ and $\Sigma(y)$ between data points (equivalently, differential distributions
are modeled as Dirac's $\delta$ functions at the position of each data point). 
\item 
As shown by Petrosian (1992), $\Phi(x)$ and $\Sigma(y)$ represent an optimal data summary. 
\item
The differential distributions $\Psi(x)$ and $\rho(y)$ can be obtained by differentiating cumulative 
distributions in the relevant axis; an approximate normalization can be obtained by requiring that 
the total predicted number of objects is equal to their observed number. 
\end{itemize}

\vskip -0.7in
\phantom{x}
%\scalebox{0.9}{\hskip 1.4in \includegraphics{figures/fig_lyndenbell_setup-1.pdf}}

\vfill
\end{slide}
 


%------------------------------------------------------------------------------
\begin{slide}

\begin{itemize}
\item {\bf astroML Book Figure 4.9:} 
The right panel shows a realization of truncated separable two-dimensional 
Gaussian distribution (with the truncation given by the solid line).
The lines in the left panel show the true one-dimensional distributions of $x$ 
and $y$, and the points are computed from the truncated data set using the $C^-$ method
(with error bars from 20 bootstrap resamples). 
\end{itemize}

\hskip -0.3in
%\begin{verbatim}
https://www.astroml.org/book\_figures/chapter4/fig\_lyndenbell\_toy.html
%\end{verbatim} 
\vskip -0.7in
\phantom{x}
\scalebox{0.9}{\hskip 0.4in \includegraphics{figures/fig_lyndenbell_toy-1.pdf}}

\vfill
\end{slide}
 

%------------------------------------------------------------------------------
% TWO-SIDED PAGE 
\begin{slide}

\hbox to \hsize{
\begin{minipage}[t]{11cm}
\begin{center}
\vskip 0.4in
\scalebox{0.7}{\hskip -1.4in \includegraphics{figures/fig_lyndenbell_gals-1.pdf}}
\end{center}
\vskip 0.3in
%\begin{verbatim}
%http://astroml.github.com/
%book_figures/chapter4/fig_lyndenbell_gals.html
%\end{verbatim} 
\hskip -0.5in
https://www.astroml.org/book\_figures/chapter4/fig\_lyndenbell\_gals.html
\end{minipage}

\begin{minipage}[t]{13cm}

\begin{itemize}
\item {\bf astroML Book Figure 4.10:}
The luminosity function for two 
    $u-r$ color-selected subsamples of SDSS galaxies from the spectroscopic sample, with 
    redshift in the range $0.08 < z < 0.12$ and flux limited to $r < 17.7$.  
\item The left
    panels show the distribution of sources as a function of redshift and
    absolute magnitude.  The distribution $p(z, M) = \rho(z)\Phi(m)$ is
    obtained using Lynden-Bell's method, with errors determined by
    20 bootstrap resamples, and  shown in the right panels.
\end{itemize}  
\end{minipage}}
\vfill 
\end{slide}

%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} Test of L-z Correlation. II}}
\end{center}
\vskip 0.2in
\hrule

\begin{itemize}
\item In reality, the selection function is typically complex:
      $s(L,z|SED, ...)$ (no sharp faint limit!)
\item First define a generalized comparable set (Fan et al. 2001;
      AJ 121, 54)
      $J_i = \{ j:L_j > L_i\}$; this is a luminosity limited data 
      subset for object $i$
\item Then generalize $N_i$ to
\begin{equation}
        T_i = \sum_{j=1}^{N_i} {s(L_i, z_j|SED_j) \over s(L_j, z_j|SED_j)},
\end{equation}
and redefine the rank accordingly
\begin{equation}
        R_i = \sum_{j=1}^{N_i} {s(L_i, z_j|SED_j) \over s(L_j, z_j|SED_j)},
\end{equation}
for $z_j < z_i$. It follows that $E(R_i) = T_i/2$ and $V(R_i) = T_i^2/12$. 

\end{itemize}

\vfill
\end{slide}
 



%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} Evolving Luminosity Function}}
\end{center}
\vskip 0.2in
\hrule

\begin{itemize}
\item The C$^-$ method is simple and optimal, but it is valid only 
      for uncorrelated variables (separable luminosity function). 
      What do we do when the $\tau$ test suggests correlated 
      variables? 
\item  Kelly, Fan and Vestergaard (2008, ApJ 682, 874)
      described a powerful and completely general Bayesian approach
      (see their Appendix A for a nice introduction to Bayesian methodology). 
      While too complex for homework, this is a fantastic method -- if you ever
      come again across the problem of estimating a general multi-dimensional
      distribution that is sampled with non-negligible and possibly 
      complex selection function, remember that paper!

\item \dots and let's now say a few words to help you with HW \# 2
\end{itemize}

\vfill
\end{slide}
 


%------------------------------------------------------------------------------
% TWO-SIDED PAGE 
\begin{slide}

\hbox to \hsize{
\begin{minipage}[t]{12cm}
\begin{center}
\vskip -0.0in
\scalebox{0.8}{\hskip -0.0in \includegraphics{figures/HW2_figure1.jpg}}
\end{center}

\end{minipage}

\begin{minipage}[t]{12cm}
\begin{center}
%{\large \color{red} ISM Medium }
\end{center}


\end{minipage}}
\vfill 
\end{slide}
%--------------------------------------------------------------------------------------------




%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} LF normalization}}
\end{center}
\vskip 0.2in
\hrule

\begin{itemize}
\item The C$^-$ method does not know (or need) details about our sample; in particular, 
     it cannot give us the overall LF normalization! 
\item We will use HW\#2 problem to discuss normalization in more detail; we can talk 
about three levels of normalization in this case:
\begin{enumerate} 
\item {\color{blue} The sample normalization:} if we didn't have the selection effects, how many 
  objects would our sample contain? 
\item {\color{blue} Normalization to the full sky:} we need to know the sky coverage for our 
   sample (and have arguments why we can extrapolate to the whole sky). 
\item {\color{blue} Extrapolation}  from the volume probed by the sample  {\color{blue} to some other position;} 
   here, we want to know LF at $z=0$. 
\end{enumerate} 
\end{itemize}

\vfill
\end{slide}
 
%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} LF normalization: the sample normalization}}
\end{center}
\vskip 0.2in
\hrule
{\color{blue} If we didn't have the selection effects, how many 
  objects would our sample contain?} 
\begin{itemize}
\item  To recap, the cumulative luminosity (absolute magnitude) function is 
$\Phi_c(M_j)$ and the cumulative distance distribution is $n_c(D_j)$ where
$j=1...N$.
\item Both $\Phi_c(M_j)$ and $n_c(D_j)$ are  {\color{blue} direct outputs} from the C$^-$ method;
{\color{blue} let us renormalize them} as $\Phi_c(M_N)=1$ and  $n_c(D_N)=1$,
where it is assumed that $M_j$ and $D_j$ arrays are sorted so that $M_N$ and 
$D_N$ are their maxima (btw, C$^-$ would return $\Phi_c(M_N)=N$ and  $n_c(D_N)=N$). 
\item The number of points, $n$,  brighter than some arbitrary $M^\ast$ and closer than
some arbitrary $D^\ast$ is then
\begin{equation}
          n(M<M^\ast \,\, {\rm and} \,\,  D<D^\ast)= C \, \Phi_c(M^\ast) \, n_c(D^\ast).
\end{equation}
where we (still) don't know $C$ (n.b. $C$ is dimensionless). 
\end{itemize}

\vfill
\end{slide}
 

%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} LF normalization: the sample normalization}}
\end{center}
%\vskip 0.2in
\hrule
\begin{itemize}
\item Now, if make sure that $M^\ast$ and $D^\ast$ are {\color{blue} within our selection volume} 
(the implied apparent mag must be above our cutoff)
and thus  {\color{blue} unaffected by selection effects,} then we get $C$ from
\begin{equation}
          N^o(M<M^o \,\, {\rm and} \,\,  D<D^o)= C \, \Phi_c(M^o) \, n_c(D^o),
\end{equation}
which is almost the same expression as on the previous page, except that here 
$n(M<M^\ast \,\, {\rm and} \,\,  D<D^\ast)$ is replaced by $N^o(M<M^o \,\, {\rm and} \,\,  D<D^o)$:
{\color{blue} the actual number of objects in our sample that satisfy this condition.}
\item This is not mathematically optimal solution for $C$ because  $N^o$ is a random variable, 
but with modern large samples this is nit-picking; the optimal procedure would integrate over the
full sample, but nevertheless would still need to adopt an interpolation procedure for 
$\Phi_c(M)$ and $n_c(D)$...
\item  {\color{blue} Given the real sample size, $N$, that is affected by selection effects,  the ``corrected'' sample
size is $C$!}
\end{itemize}
\vfill
\end{slide}
 

%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} LF normalization: the sample normalization}}
\end{center}
\vskip 0.2in
\hrule
\begin{itemize} 
\item The number of points per unit two-dimensional area, $dA = dM \, dD$, is then
\begin{equation}
  {d^2 N \over dM dD } = C \, \left( {d \Phi_c(M) \over dM } \right) \, \left( {d n_c(D) \over d D} \right),
\end{equation}
where we now know $C$ and can easily take (numerical) derivatives  ${d \Phi_c(M) / dM }$ and
${d n_c(D) / d D}$ (where $\Phi_c(M)$ and $n_c(D)$ came from $C^-$ and are normalized to 1). 
\item The quantities in parenthesis are differential distribution functions.
\item  When normalizing to the full sky (step \#2), we need to know the fraction of sky, $f_{sky}$,
covered by our sample; if justified, we need to multiply $C$ by $1/f_{sky}$: $C_{sky} = C / f_{sky}$. 
\end{itemize}

\vfill
\end{slide}
 


 
%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} LF normalization: extrapolation}}
\end{center}
\vskip 0.2in
\hrule
{\color{blue} How do we go from $d^2 N / (dM dD)$ to volume density? } 
\begin{equation}
  \Phi(M,D) \equiv  {d^2 N \over dM dV } =    {d^2 N \over dM dD }  \, {dD \over dV},
\end{equation}
where $dV = 4\pi D^2 dD$. We have two cases of interest: 
\begin{itemize}
\item {\bf Case 1:} We seek the volume density vs. D, $\rho(D)$, and we don't care about $M$ distribution:
\begin{equation}
   \rho(D) = \int \Phi(M,D) dM = {C_{sky} \over 4\pi D^2} \, \left( {d n_c(D) \over d D} \right),
\end{equation}
where we used the fact that $\int_{-\infty}^\infty  \left( {d \Phi_c(M) \over dM } \right)   dM = \Phi_c(M_N)=1$. 
\item Unit for $\rho(D)$ is the number of objects per (distance unit)$^3$ (remember that 
$n_c(D)$ was dimensionless and normalized to unity at $D=D_N$; the unit comes from 
taking derivative with respect to $D$, $d n_c(D) /d D$, and from the 1/$D^2$ term).  
\end{itemize}

\vfill
\end{slide}
 
%------------------------------------------------------------------------------
\begin{slide}
\begin{center}
\bfseries
{\large {\color{red} LF normalization: extrapolation}}
\end{center}
\vskip 0.2in
\hrule
\begin{itemize}
\item Given $\rho(D)$, we can fit some function to it and {\bf extrapolate} to get $\rho(D=D_0)$
(and thus the ratio $\rho(D)/\rho(D=D_0)$ for any $D_0$, including $\rho(D_0)/\rho(D_N)$). 
\item {\bf Case 2:} We want to know the $M$ distribution at some $D=D_0$, call it  $\psi(M | D=D_0) $ 
(e.g. $D_0=0$ corresponding to solar neighborhood, as discussed in this HW). First, at $D=D_N$
(recall  $n_c(D_N)=1$)
\begin{equation}
   \psi(M | D=D_N) = \int \Phi(M,D) dD = C_{sky} \, \left( {d \Phi_c(M) \over dM } \right). 
\end{equation}
\item Then, extrapolating to $D_0$ (unit for $\psi$ is the number of objects per mag; this
is what we compare to the ``true'' LF)
\begin{equation}
   \psi(M | D=D_0) =  \psi(M | D=D_N) \, {\rho(D_0)  \over \rho(D_N) }.
\end{equation}
\end{itemize}

\vfill
\end{slide}
 

 





  

%%%%%%%%%%%%%%%%
\end{document}

